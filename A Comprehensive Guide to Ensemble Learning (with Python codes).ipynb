{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Ensemble Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, you can use “VotingClassifier” module in sklearn as follows:\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = tree.DecisionTreeClassifier(random_state=1)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "model.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1+pred2+pred3)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Ensemble techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stacking(model,train,y,test,n_fold):\n",
    "   folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "   test_pred=np.empty((test.shape[0],1),float)\n",
    "   train_pred=np.empty((0,1),float)\n",
    "   for train_indices,val_indices in folds.split(train,y.values):\n",
    "      x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
    "      y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
    "\n",
    "      model.fit(X=x_train,y=y_train)\n",
    "      train_pred=np.append(train_pred,model.predict(x_val))\n",
    "      test_pred=np.append(test_pred,model.predict(test))\n",
    "    return test_pred.reshape(-1,1),train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we’ll create two base models – decision tree and knn.\n",
    "\n",
    "model1 = tree.DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)\n",
    "\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "\n",
    "\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "\n",
    "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train)\n",
    "\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a third model, logistic regression, on the predictions of the decision tree and knn models.\n",
    "\n",
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(df,y_train)\n",
    "model.score(df_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model1.fit(x_train, y_train)\n",
    "val_pred1=model1.predict(x_val)\n",
    "test_pred1=model1.predict(x_test)\n",
    "val_pred1=pd.DataFrame(val_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "model2.fit(x_train,y_train)\n",
    "val_pred2=model2.predict(x_val)\n",
    "test_pred2=model2.predict(x_test)\n",
    "val_pred2=pd.DataFrame(val_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)\n",
    "df_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(df_val,y_val)\n",
    "model.score(df_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms based on Bagging and Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#reading the dataset\n",
    "df=pd.read_csv(\"/home/user/Desktop/train.csv\")\n",
    "\n",
    "#filling missing values\n",
    "df['Gender'].fillna('Male', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=0)\n",
    "\n",
    "x_train=train.drop('Loan_Status',axis=1)\n",
    "y_train=train['Loan_Status']\n",
    "\n",
    "x_test=test.drop('Loan_Status',axis=1)\n",
    "y_test=test['Loan_Status']\n",
    "\n",
    "#create dummies\n",
    "x_train=pd.get_dummies(x_train)\n",
    "x_test=pd.get_dummies(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging meta-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model= RandomForestClassifier(random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in sorted(zip(x_train.columns, model.feature_importances_)):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model= RandomForestRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (GBM)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model= GradientBoostingRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model=xgb.XGBRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "train_data=lgb.Dataset(x_train,label=y_train)\n",
    "#define parameters\n",
    "params = {'learning_rate':0.001}\n",
    "model= lgb.train(params, train_data, 100) \n",
    "y_pred=model.predict(x_test)\n",
    "for i in range(0,185):\n",
    "   if y_pred[i]>=0.5: \n",
    "   y_pred[i]=1\n",
    "else: \n",
    "   y_pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "train_data=lgb.Dataset(x_train,label=y_train)\n",
    "params = {'learning_rate':0.001}\n",
    "model= lgb.train(params, train_data, 100)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse=mean_squared_error(y_pred,y_test)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "model=CatBoostClassifier()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "model.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "model=CatBoostRegressor()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "model.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\n",
    "model.score(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
